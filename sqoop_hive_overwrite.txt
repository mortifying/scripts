"sqoop import -m 1 --connect \"jdbc:%s://%s:%s;database=%s;username=%s;password=%s\" --fields-terminated-by \"\\t\" --query \"select %s from %s 
where \$CONDITIONS and 1=1\"  --target-dir %s --hive-import --hive-database %s --hive-overwrite --hive-table %s --delete-target-dir --mapreduce-job-name %s"
 
%(datasource['datasource_type'],datasource['datasource_ip'],datasource['datasource_port'],jobpara['database_name'],datasource['datasource_username'],
datasource['datasource_password'],jobpara['database_field'],jobpara['database_table'],jobpara['job_name'],jobpara['hive_database_name'],jobpara['hive_database_table'],
jobpara['hive_database_table'])







"sqoop import --append --connect \"jdbc:%s://%s:%s;database=%s;username=%s;password=%s\" --fields-terminated-by \"\\t\" -m 3 --query \"select %s from %s 
where MonitorTime between '%s01' and dateadd(ms,-3,DATEADD(mm, DATEDIFF(m,0,'%s01')+1, 0)) and \$CONDITIONS and 1=1\" --split-by monitortime 
--target-dir \"hdfs://cdh-0:8020/user/hive/warehouse/%s.db/%s/%s=%s\" --mapreduce-job-name %s" 





% (datasource['datasource_type'],datasource['datasource_ip'],datasource['datasource_port'],jobpara['database_name'],datasource['datasource_username'],
datasource['datasource_password'],jobpara['database_field'],jobpara['database_table'],partitionkey,partitionkey,jobpara['hive_database_name'],
jobpara['hive_database_table'],jobpara['pritition_type'],partitionkey,jobpara['hive_database_table'])




sqoop import --append --connect "jdbc:sqlserver://10.87.10.65:1433;database=AutoMonitorDBV3;username=sa;password=Jkzx2008" --fields-terminated-by "\t" -m 3 
--query "select PSCode,OutputCode,MonitorTime,MinFlow,AvgFlow,MaxFlow,RevisedFlow,CouFlow,UpdateDate,IsException,TyperUnit,TyperName,DataSource,AvailableStatus,
ReviseCause,LastExamineFlag,RevisedTime,RevisedAvgFlow,ExchangeDate from MonitorOperationData.WaterOutputHourData where MonitorTime between '20180701' and 
dateadd(ms,-3,DATEADD(mm, DATEDIFF(m,0,'20180701')+1, 0)) and \$CONDITIONS and 1=1" --split-by monitortime 
--target-dir "hdfs://cdh-0:8020/user/hive/warehouse/stage_cems_water.db/s_wateroutputhourdata/month=201807" --mapreduce-job-name s_wateroutputhourdata



--hive-partition-key year --hive-partition-value 2018 --delete-target-dir
